{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Class Learning on German Credit Risk Assessment\n",
    "\n",
    "## Hatice Erdogan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "Credit risk assessment influences financial decisions, such as loan approvals, interest rates, and credit limits. Understanding one's credit risk is pivotal for making informed financial choices.\n",
    "\n",
    "For banks and financial institutions, accurately assessing credit risk is foundational to wise lending practices. It ensures the sustainability of their operations and helps prevent financial crises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "\n",
    "- The dataset is retreived from UCI Machine Learning Repository titled as German Credit Data\n",
    "- Individuals classified based on a set of attributes, such as, status of existing checking account, credit history, savings account, present employment since, marital status & sex, housing, job, other debtors/gurantors, total of 24 features that categorizes individuals as either good or bad credit risk.\n",
    "- The dataset was already converted into numerical format so it was already available to use for machine learning models. \n",
    "- Total of 1000 instances; 700 good, 300 bad \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  15  16  17  18  19  20  21  \\\n",
       "0   1   6   4  12   5   5   3   4   1  67  ...   0   0   1   0   0   1   0   \n",
       "1   2  48   2  60   1   3   2   2   1  22  ...   0   0   1   0   0   1   0   \n",
       "2   4  12   4  21   1   4   3   3   1  49  ...   0   0   1   0   0   1   0   \n",
       "3   1  42   2  79   1   4   3   4   2  45  ...   0   0   0   0   0   0   0   \n",
       "4   1  24   3  49   1   3   3   4   4  53  ...   1   0   1   0   0   0   0   \n",
       "\n",
       "   22  23  24  \n",
       "0   0   1   1  \n",
       "1   0   1   2  \n",
       "2   1   0   1  \n",
       "3   0   1   1  \n",
       "4   0   1   2  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccard = pd.read_csv(\"german_creditcard.csv\", header = None) \n",
    "ccard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column is the target variable. It is classified as 1 = \"good\" and 2 = \"bad\", but it needs to be converted into 0 = \"good\" and 1 = \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforn target variable into 0's and 1's\n",
    "ccard.iloc[:, -1] = ccard.iloc[:, -1].apply(lambda x: 1 if x == 2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    700\n",
       "1    300\n",
       "Name: 24, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkk the number of observations classified as 0 and 1 \n",
    "ccard.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 700 normal cases and 300 anomaly cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25)\n",
      "(1000, 24)\n",
      "(1000,)\n",
      "[0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1\n",
      " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
      " 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0\n",
      " 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0\n",
      " 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "# Shape of the entire dataset\n",
    "print(np.shape(ccard))\n",
    "\n",
    "X = ccard.iloc[:,0:24].values\n",
    "print(np.shape(X))\n",
    "\n",
    "y = ccard.iloc[:,24].values\n",
    "print(np.shape(y))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Class Learning Approach for Credit Risk Assessment\n",
    "\n",
    "In this project, a distinctive approach known as one-class learning was employed for analyzing the credit risk dataset. Unlike traditional methods that predict whether an individual is a good or bad credit risk, one-class learning focuses on training the model solely on normal classes and subsequently testing it on both normal and anomaly classes.\n",
    "\n",
    "The methodology involved the following steps:\n",
    "\n",
    "### Data Preprocessing:\n",
    "\n",
    "The dataset was initially categorized into negative-only (0, indicating good credit) and positive-only (1, indicating bad credit) classes for both feature variables (X) and the target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 24)\n",
      "(700,)\n",
      "(300, 24)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "X_neg_only = X[y == 0]\n",
    "y_neg_only = y[y == 0]\n",
    "print(np.shape(X_neg_only))\n",
    "print(np.shape(y_neg_only))\n",
    "\n",
    "X_pos_only = X[y == 1]\n",
    "y_pos_only = y[y == 1]\n",
    "print(np.shape(X_pos_only))\n",
    "print(np.shape(y_pos_only))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_neg_only, y_neg_only, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting:\n",
    "\n",
    "The dataset was divided into training and testing sets, utilizing only the negative class (0) to train the model. This emphasizes learning from instances of good credit.\n",
    "\n",
    "### Model Evaluation:\n",
    "\n",
    "The trained model was then tested on both the negative (good credit) and positive (bad credit) classes to assess its performance in distinguishing between normal and anomalous instances.\n",
    "This one-class learning approach aims to enhance the model's ability to identify potential credit risks by focusing on the characteristics of good credit instances during the training phase. The inclusion of both positive and negative classes in the testing phase allows for a comprehensive evaluation of the model's generalization to different credit risk scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 24)\n",
      "(140, 24)\n",
      "(140,)\n",
      "(440, 24)\n",
      "140\n",
      "300\n",
      "(440,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "# Adding the positive labels to full X_test\n",
    "X_test_both = np.vstack((X_test, X_pos_only))\n",
    "print(np.shape(X_test_both))\n",
    "\n",
    "print(len(y_test))\n",
    "print(len(y_pos_only))\n",
    "\n",
    "# Adding the positive labels to full t_test\n",
    "y_test_both = np.concatenate((y_test, y_pos_only))\n",
    "print(np.shape(y_test_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methadology\n",
    "\n",
    "To effectively assess credit risk using the one-class learning approach, three distinct anomaly detection algorithms—Local Outlier Factor, Isolation Forest, and One-Class Support Vector Machine (SVM)—were implemented. The primary goal was to evaluate their performance on key metrics such as recall, precision, F-1 score, and ROC-AUC. The metrics were computed using three different aggregation methods: micro, macro, and weighted.\n",
    "\n",
    "### Performance Evaluation:\n",
    "\n",
    "The implemented algorithms were rigorously evaluated based on the following metrics:\n",
    "\n",
    "- Accuracy: The ratio of correctly predicted instances to the total number of instances, offering a comprehensive measure of overall model correctness.\n",
    "- Recall: The ability of the model to correctly identify true positives among all actual positive instances.\n",
    "- Precision: The proportion of correctly identified positive instances among all instances predicted as positive.\n",
    "- F-1 Score: The harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
    "- ROC-AUC Score: The Area Under the Receiver Operating Characteristic curve, assessing the model's ability to distinguish between positive and negative instances across different thresholds.\n",
    "\n",
    "### Aggregation Methods:\n",
    "\n",
    "The computed metrics were aggregated using three different methods:\n",
    "\n",
    "- Macro: Calculating metrics independently for each class and then averaging them.\n",
    "- Weighted: Computing a weighted average based on the number of instances in each class.\n",
    "This comprehensive evaluation strategy aims to provide insights into the strengths and weaknesses of each algorithm under consideration, facilitating the selection of the most suitable approach for credit risk assessment based on the defined evaluation criteria.\n",
    "\n",
    "### Local Outlier Factor (LOF):\n",
    "\n",
    "LOF, a density-based anomaly detection algorithm, was implemented with varying parameters to identify potential outliers in the credit risk dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors: 5\n",
      "Precision: 0.7851719046614638\n",
      "Recall: 0.3386363636363636\n",
      "F1 Score: 0.19574367448136853\n",
      "\n",
      "Neighbors: 10\n",
      "Precision: 0.7854122621564482\n",
      "Recall: 0.3409090909090909\n",
      "F1 Score: 0.20028811030508822\n",
      "\n",
      "Neighbors: 15\n",
      "Precision: 0.7856537401991947\n",
      "Recall: 0.3431818181818182\n",
      "F1 Score: 0.20480604392697027\n",
      "\n",
      "Neighbors: 20\n",
      "Precision: 0.7863849765258216\n",
      "Recall: 0.35\n",
      "F1 Score: 0.21820350656073464\n",
      "\n",
      "Neighbors: 25\n",
      "Precision: 0.7868782161234992\n",
      "Recall: 0.35454545454545455\n",
      "F1 Score: 0.22700748394257686\n",
      "\n",
      "Neighbors: 50\n",
      "Precision: 0.7636534839924669\n",
      "Recall: 0.375\n",
      "F1 Score: 0.2683776564858901\n",
      "\n",
      "Neighbors: 100\n",
      "Precision: 0.7411794452872759\n",
      "Recall: 0.3977272727272727\n",
      "F1 Score: 0.31370682103819936\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Neighbors</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Macro)</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1 Score (Macro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F1 Score (Weighted)</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.338636</td>\n",
       "      <td>0.662413</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.274310</td>\n",
       "      <td>0.785172</td>\n",
       "      <td>0.338636</td>\n",
       "      <td>0.195744</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.277872</td>\n",
       "      <td>0.785412</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.200288</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.343182</td>\n",
       "      <td>0.663170</td>\n",
       "      <td>0.518333</td>\n",
       "      <td>0.281415</td>\n",
       "      <td>0.785654</td>\n",
       "      <td>0.343182</td>\n",
       "      <td>0.204806</td>\n",
       "      <td>0.481667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.664319</td>\n",
       "      <td>0.523333</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>0.786385</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.218204</td>\n",
       "      <td>0.476667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.354545</td>\n",
       "      <td>0.665094</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.298860</td>\n",
       "      <td>0.786878</td>\n",
       "      <td>0.354545</td>\n",
       "      <td>0.227007</td>\n",
       "      <td>0.473333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.649762</td>\n",
       "      <td>0.539762</td>\n",
       "      <td>0.330867</td>\n",
       "      <td>0.763653</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.268378</td>\n",
       "      <td>0.460238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>0.397727</td>\n",
       "      <td>0.635094</td>\n",
       "      <td>0.552619</td>\n",
       "      <td>0.365611</td>\n",
       "      <td>0.741179</td>\n",
       "      <td>0.397727</td>\n",
       "      <td>0.313707</td>\n",
       "      <td>0.447381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N Neighbors  Accuracy  Precision (Macro)  Recall (Macro)  F1 Score (Macro)  \\\n",
       "0            5  0.338636           0.662413        0.515000          0.274310   \n",
       "1           10  0.340909           0.662791        0.516667          0.277872   \n",
       "2           15  0.343182           0.663170        0.518333          0.281415   \n",
       "3           20  0.350000           0.664319        0.523333          0.291936   \n",
       "4           25  0.354545           0.665094        0.526667          0.298860   \n",
       "5           50  0.375000           0.649762        0.539762          0.330867   \n",
       "6          100  0.397727           0.635094        0.552619          0.365611   \n",
       "\n",
       "   Precision (Weighted)  Recall (Weighted)  F1 Score (Weighted)   ROC-AUC  \n",
       "0              0.785172           0.338636             0.195744  0.485000  \n",
       "1              0.785412           0.340909             0.200288  0.483333  \n",
       "2              0.785654           0.343182             0.204806  0.481667  \n",
       "3              0.786385           0.350000             0.218204  0.476667  \n",
       "4              0.786878           0.354545             0.227007  0.473333  \n",
       "5              0.763653           0.375000             0.268378  0.460238  \n",
       "6              0.741179           0.397727             0.313707  0.447381  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local Outlier Factor\n",
    "\n",
    "n_neig = [5, 10, 15, 20, 25, 50, 100]\n",
    "result_lof =[]\n",
    "\n",
    "for k in n_neig:\n",
    "    lof = LocalOutlierFactor(n_neighbors = k, novelty = True)\n",
    "    lof.fit(X_train)\n",
    "    preds = lof.predict(X_test_both)\n",
    "    #print(preds)\n",
    "\n",
    "    preds_new = np.where(preds, preds == 1, 0)\n",
    "    preds_new = np.where(preds_new, preds_new == -1, 1)\n",
    "    #print(preds_new)\n",
    "    \n",
    "    print(\"Neighbors:\", k)\n",
    "    \n",
    "    print(\"Precision:\", precision_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print(\"Recall:\", recall_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print(\"F1 Score:\", f1_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_test_both, preds_new, target_names=['Good', 'Bad'], output_dict=True)\n",
    "    \n",
    "    resultlof_dict = {\n",
    "        \"N Neighbors\": k,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision (Macro)\": report['macro avg']['precision'],\n",
    "        \"Recall (Macro)\": report['macro avg']['recall'],\n",
    "        \"F1 Score (Macro)\": report['macro avg']['f1-score'],\n",
    "        \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
    "        \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
    "        \"F1 Score (Weighted)\": report['weighted avg']['f1-score'],\n",
    "        \"ROC-AUC\": roc_auc_score(y_test_both, preds)\n",
    "    }\n",
    "\n",
    "    result_lof.append(resultlof_dict)\n",
    "\n",
    "# Create a DataFrame\n",
    "resultlof_df = pd.DataFrame(result_lof)\n",
    "\n",
    "# Display the DataFrame\n",
    "resultlof_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest:\n",
    "\n",
    "The Isolation Forest algorithm, known for its ability to isolate anomalies efficiently, was applied with different configurations to capture the peculiarities of credit risk instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 50\n",
      "Precision: 0.6660405709992486\n",
      "Recall: 0.5772727272727273\n",
      "F1 Score: 0.5904187908224999\n",
      "\n",
      "Estimator: 100\n",
      "Precision: 0.6617382849654415\n",
      "Recall: 0.5613636363636364\n",
      "F1 Score: 0.5735355656982065\n",
      "\n",
      "Estimator: 150\n",
      "Precision: 0.6473829201101929\n",
      "Recall: 0.5363636363636364\n",
      "F1 Score: 0.547126141185547\n",
      "\n",
      "Estimator: 200\n",
      "Precision: 0.6539902534377673\n",
      "Recall: 0.5477272727272727\n",
      "F1 Score: 0.5592250329092434\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Estimators</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Macro)</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1 Score (Macro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F1 Score (Weighted)</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.596419</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.569798</td>\n",
       "      <td>0.666041</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.590419</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.561364</td>\n",
       "      <td>0.590677</td>\n",
       "      <td>0.602143</td>\n",
       "      <td>0.556301</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.561364</td>\n",
       "      <td>0.573536</td>\n",
       "      <td>0.397857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.583810</td>\n",
       "      <td>0.533239</td>\n",
       "      <td>0.647383</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.547126</td>\n",
       "      <td>0.416190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>0.547727</td>\n",
       "      <td>0.582553</td>\n",
       "      <td>0.592143</td>\n",
       "      <td>0.543766</td>\n",
       "      <td>0.653990</td>\n",
       "      <td>0.547727</td>\n",
       "      <td>0.559225</td>\n",
       "      <td>0.407857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N Estimators  Accuracy  Precision (Macro)  Recall (Macro)  \\\n",
       "0            50  0.577273           0.596419        0.610000   \n",
       "1           100  0.561364           0.590677        0.602143   \n",
       "2           150  0.536364           0.575758        0.583810   \n",
       "3           200  0.547727           0.582553        0.592143   \n",
       "\n",
       "   F1 Score (Macro)  Precision (Weighted)  Recall (Weighted)  \\\n",
       "0          0.569798              0.666041           0.577273   \n",
       "1          0.556301              0.661738           0.561364   \n",
       "2          0.533239              0.647383           0.536364   \n",
       "3          0.543766              0.653990           0.547727   \n",
       "\n",
       "   F1 Score (Weighted)   ROC-AUC  \n",
       "0             0.590419  0.390000  \n",
       "1             0.573536  0.397857  \n",
       "2             0.547126  0.416190  \n",
       "3             0.559225  0.407857  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolation Forest \n",
    "n_est = [50, 100, 150, 200]\n",
    "result_iso = []\n",
    "\n",
    "for est in n_est:\n",
    "    isf = IsolationForest(random_state=0, n_estimators = est).fit(X_train)\n",
    "    preds = isf.predict(X_test_both)\n",
    "    #print(preds)\n",
    "\n",
    "    preds_new = np.where(preds, preds == 1, 0)\n",
    "    preds_new = np.where(preds_new, preds_new == -1, 1)\n",
    "    #print(preds_new)\n",
    "\n",
    "    print(\"Estimator:\", est)\n",
    "    print(\"Precision:\", precision_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print(\"Recall:\", recall_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print(\"F1 Score:\", f1_score(y_test_both, preds_new, average = \"weighted\"))   \n",
    "    print()\n",
    "\n",
    "# Classification Report\n",
    "    report = classification_report(y_test_both, preds_new, target_names=['Good', 'Bad'], output_dict=True)\n",
    "    \n",
    "    resultiso_dict = {\n",
    "        \"N Estimators\": est,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision (Macro)\": report['macro avg']['precision'],\n",
    "        \"Recall (Macro)\": report['macro avg']['recall'],\n",
    "        \"F1 Score (Macro)\": report['macro avg']['f1-score'],\n",
    "        \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
    "        \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
    "        \"F1 Score (Weighted)\": report['weighted avg']['f1-score'],\n",
    "        \"ROC-AUC\": roc_auc_score(y_test_both, preds)\n",
    "    }\n",
    "\n",
    "    result_iso.append(resultiso_dict)\n",
    "\n",
    "# Create a DataFrame\n",
    "resultiso_df = pd.DataFrame(result_iso)\n",
    "\n",
    "# Display the DataFrame\n",
    "resultiso_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Class Support Vector Machine (SVM):\n",
    "\n",
    "The One-Class SVM, a support vector machine variant designed for one-class classification, was employed to discern normal credit instances from potential risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "Precision: 0.5744203412043493\n",
      "Recall: 0.4863636363636364\n",
      "F1 Score: 0.5017821011821557\n",
      "\n",
      "Kernel: poly\n",
      "Precision: 0.57772974230569\n",
      "Recall: 0.48863636363636365\n",
      "F1 Score: 0.5038359442340763\n",
      "\n",
      "Kernel: rbf\n",
      "Precision: 0.6307784477427336\n",
      "Recall: 0.6772727272727272\n",
      "F1 Score: 0.6197605125166311\n",
      "\n",
      "Kernel: sigmoid\n",
      "Precision: 0.4648760330578512\n",
      "Recall: 0.6818181818181818\n",
      "F1 Score: 0.5528255528255529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Macro)</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1 Score (Macro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F1 Score (Weighted)</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.507962</td>\n",
       "      <td>0.509048</td>\n",
       "      <td>0.478509</td>\n",
       "      <td>0.574420</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.501782</td>\n",
       "      <td>0.490952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.511118</td>\n",
       "      <td>0.512619</td>\n",
       "      <td>0.481108</td>\n",
       "      <td>0.577730</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.503836</td>\n",
       "      <td>0.487381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.677273</td>\n",
       "      <td>0.590349</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.519739</td>\n",
       "      <td>0.630778</td>\n",
       "      <td>0.677273</td>\n",
       "      <td>0.619761</td>\n",
       "      <td>0.459524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.464876</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.552826</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Kernel  Accuracy  Precision (Macro)  Recall (Macro)  F1 Score (Macro)  \\\n",
       "0   linear  0.486364           0.507962        0.509048          0.478509   \n",
       "1     poly  0.488636           0.511118        0.512619          0.481108   \n",
       "2      rbf  0.677273           0.590349        0.540476          0.519739   \n",
       "3  sigmoid  0.681818           0.340909        0.500000          0.405405   \n",
       "\n",
       "   Precision (Weighted)  Recall (Weighted)  F1 Score (Weighted)   ROC-AUC  \n",
       "0              0.574420           0.486364             0.501782  0.490952  \n",
       "1              0.577730           0.488636             0.503836  0.487381  \n",
       "2              0.630778           0.677273             0.619761  0.459524  \n",
       "3              0.464876           0.681818             0.552826  0.500000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Class SVM\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "results = []\n",
    "\n",
    "for ker in kernels:\n",
    "    ocsvm = OneClassSVM(gamma='auto', kernel=ker).fit(X_train)\n",
    "    preds = ocsvm.predict(X_test_both)\n",
    "#    print(preds)\n",
    "\n",
    "    preds_new = np.where(preds, preds == 1, 0)\n",
    "    preds_new = np.where(preds_new, preds_new == -1, 1)\n",
    "#    print(preds_new)\n",
    "\n",
    "    print(\"Kernel:\", ker)\n",
    "    print(\"Precision:\", precision_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print(\"Recall:\", recall_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print(\"F1 Score:\", f1_score(y_test_both, preds_new, average = \"weighted\"))\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_test_both, preds_new, target_names=['Good', 'Bad'], output_dict=True)\n",
    "    \n",
    "    result_dict = {\n",
    "        \"Kernel\": ker,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision (Macro)\": report['macro avg']['precision'],\n",
    "        \"Recall (Macro)\": report['macro avg']['recall'],\n",
    "        \"F1 Score (Macro)\": report['macro avg']['f1-score'],\n",
    "        \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
    "        \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
    "        \"F1 Score (Weighted)\": report['weighted avg']['f1-score'],\n",
    "        \"ROC-AUC\": roc_auc_score(y_test_both, preds)\n",
    "    }\n",
    "\n",
    "    results.append(result_dict)\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis\n",
    "\n",
    "After implementing Local Outlier Factor (LOF), Isolation Forest, and One-Class Support Vector Machine (SVM) with various parameters, a thorough analysis of their performance metrics revealed valuable insights.\n",
    "\n",
    "Baseline Model - Isolation Forest:\n",
    "\n",
    "Isolation Forest, although performing the worst among the three models, was selected as the baseline model. Its optimal configuration, with 50 estimators, demonstrated a weighted precision score of 0.666.\n",
    "\n",
    "One-Class SVM (RBF Kernel):\n",
    "\n",
    "Among the SVM variants, the One-Class SVM with an RBF kernel emerged as the top performer. It achieved the highest recall (weighted) score of 0.681 and highest precision (weighted) score of 0.630. This suggests its effectiveness in identifying instances of credit risk, especially those that might be missed by other models.\n",
    "\n",
    "Local Outlier Factor (LOF):\n",
    "\n",
    "Local Outlier Factor exhibited the best overall performance among the three models. The optimal configuration of Local Outlier Factor (LOF), with 25 neighbors, yielded a remarkable weighted precision score of 0.786. This suggests that LOF excelled in accurately classifying instances, particularly in identifying credit risk cases. The high weighted precision indicates the model's proficiency in minimizing false positives within the credit risk category, showcasing its robustness in correctly identifying instances of anomaly or credit risk, which is crucial for effective credit risk assessment.\n",
    "\n",
    "These findings indicate that, despite the Isolation Forest serving as the baseline model, both the One-Class SVM with an RBF kernel and Local Outlier Factor outperformed it in specific metrics. The weighted precision of 0.786 achieved by LOF showcases its superiority in handling credit risk scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Model Evaluation with COPOD and HBOS\n",
    "\n",
    "The analysis was further enriched by incorporating COPOD and HBOS models from the prod library. These models were tested with various parameters to assess their performance in the context of credit risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyod\n",
    "from pyod import model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPOD Model:\n",
    "\n",
    "Cluster-based Outlier Factor with Population Outlier Detection algorithm combines the concepts of cluster-based outlier factors and population outlier detection to identify outliers in a dataset. The COPOD model was implemented with different parameter configurations to identify its optimal settings. Evaluation metrics such as accuracy, recall, precision, F-1, and ROC-AUC were computed, utilizing macro, and weighted aggregation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination: 0.05\n",
      "Precision: 0.7487433577480971\n",
      "Recall: 0.35454545454545455\n",
      "F1 Score: 0.2302912289483162\n",
      "\n",
      "Contamination: 0.1\n",
      "Precision: 0.6402746656035632\n",
      "Recall: 0.3840909090909091\n",
      "F1 Score: 0.3105814580294732\n",
      "\n",
      "Contamination: 0.15\n",
      "Precision: 0.6500911092938266\n",
      "Recall: 0.4159090909090909\n",
      "F1 Score: 0.36893435841672595\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contamination Values</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Macro)</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1 Score (Macro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F1 Score (Weighted)</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.354545</td>\n",
       "      <td>0.636914</td>\n",
       "      <td>0.524762</td>\n",
       "      <td>0.300790</td>\n",
       "      <td>0.748743</td>\n",
       "      <td>0.354545</td>\n",
       "      <td>0.230291</td>\n",
       "      <td>0.524762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.384091</td>\n",
       "      <td>0.557967</td>\n",
       "      <td>0.527381</td>\n",
       "      <td>0.357816</td>\n",
       "      <td>0.640275</td>\n",
       "      <td>0.384091</td>\n",
       "      <td>0.310581</td>\n",
       "      <td>0.527381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.415909</td>\n",
       "      <td>0.567560</td>\n",
       "      <td>0.543095</td>\n",
       "      <td>0.402044</td>\n",
       "      <td>0.650091</td>\n",
       "      <td>0.415909</td>\n",
       "      <td>0.368934</td>\n",
       "      <td>0.543095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Contamination Values  Accuracy  Precision (Macro)  Recall (Macro)  \\\n",
       "0                  0.05  0.354545           0.636914        0.524762   \n",
       "1                  0.10  0.384091           0.557967        0.527381   \n",
       "2                  0.15  0.415909           0.567560        0.543095   \n",
       "\n",
       "   F1 Score (Macro)  Precision (Weighted)  Recall (Weighted)  \\\n",
       "0          0.300790              0.748743           0.354545   \n",
       "1          0.357816              0.640275           0.384091   \n",
       "2          0.402044              0.650091           0.415909   \n",
       "\n",
       "   F1 Score (Weighted)   ROC-AUC  \n",
       "0             0.230291  0.524762  \n",
       "1             0.310581  0.527381  \n",
       "2             0.368934  0.543095  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "# Parameters to try\n",
    "contamination_values = [0.05, 0.1, 0.15]\n",
    "results_copod = []\n",
    "\n",
    "for contamination in contamination_values:\n",
    "    copod_model = COPOD(contamination=contamination)\n",
    "    copod_model.fit(X_train)\n",
    "    preds = copod_model.predict(X_test_both)\n",
    "\n",
    "    print(f\"Contamination: {contamination}\")\n",
    "    print(\"Precision:\", precision_score(y_test_both, preds, average = \"weighted\"))\n",
    "    print(\"Recall:\", recall_score(y_test_both, preds, average = \"weighted\"))\n",
    "    print(\"F1 Score:\", f1_score(y_test_both, preds, average = \"weighted\"))\n",
    "    print()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_test_both, preds, target_names=['Good', 'Bad'], output_dict=True)\n",
    "    \n",
    "    resultcp_dict = {\n",
    "        \"Contamination Values\": contamination,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision (Macro)\": report['macro avg']['precision'],\n",
    "        \"Recall (Macro)\": report['macro avg']['recall'],\n",
    "        \"F1 Score (Macro)\": report['macro avg']['f1-score'],\n",
    "        \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
    "        \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
    "        \"F1 Score (Weighted)\": report['weighted avg']['f1-score'],\n",
    "        \"ROC-AUC\": roc_auc_score(y_test_both, preds)\n",
    "    }\n",
    "\n",
    "    results_copod.append(resultcp_dict)\n",
    "\n",
    "# Create a DataFrame\n",
    "results_cp = pd.DataFrame(results_copod)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For COPOD model, accuracy increased with higher contamination values. Precision (weighted) was highest 0.75 at 0.05 contamination value. ROC-AUC also increased with higher contamination values however, precision decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HBOS Model:\n",
    "\n",
    "Histogram-based Outlier Score is an unsupervised anomaly detection algorithm that calculates an outlier score based on the histogram of feature values. The HBOS model, known for its simplicity and efficiency, was also employed with varying parameters to explore its performance. Similar to other models, performance metrics were assessed using macro, and weighted aggregation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination: 0.05, Bins: 20\n",
      "Precision: 0.6595617188837527\n",
      "Recall: 0.3568181818181818\n",
      "F1 Score: 0.24709409740184335\n",
      "\n",
      "Contamination: 0.05, Bins: 30\n",
      "Precision: 0.6855846601609313\n",
      "Recall: 0.3613636363636364\n",
      "F1 Score: 0.252414987172855\n",
      "\n",
      "Contamination: 0.05, Bins: 40\n",
      "Precision: 0.6444904621345598\n",
      "Recall: 0.3568181818181818\n",
      "F1 Score: 0.2500901423170412\n",
      "\n",
      "Contamination: 0.1, Bins: 20\n",
      "Precision: 0.6190041233884858\n",
      "Recall: 0.375\n",
      "F1 Score: 0.29796728883708634\n",
      "\n",
      "Contamination: 0.1, Bins: 30\n",
      "Precision: 0.6133119753809408\n",
      "Recall: 0.3886363636363636\n",
      "F1 Score: 0.3291877807439615\n",
      "\n",
      "Contamination: 0.1, Bins: 40\n",
      "Precision: 0.6155107778819119\n",
      "Recall: 0.37727272727272726\n",
      "F1 Score: 0.30414944903581265\n",
      "\n",
      "Contamination: 0.15, Bins: 20\n",
      "Precision: 0.6070270942303765\n",
      "Recall: 0.4113636363636364\n",
      "F1 Score: 0.3765343419572393\n",
      "\n",
      "Contamination: 0.15, Bins: 30\n",
      "Precision: 0.6139393939393939\n",
      "Recall: 0.41818181818181815\n",
      "F1 Score: 0.3862137862137862\n",
      "\n",
      "Contamination: 0.15, Bins: 40\n",
      "Precision: 0.5971586882288512\n",
      "Recall: 0.4022727272727273\n",
      "F1 Score: 0.36343243414916265\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contamination Values</th>\n",
       "      <th>Bin Values</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Macro)</th>\n",
       "      <th>Recall (Macro)</th>\n",
       "      <th>F1 Score (Macro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F1 Score (Weighted)</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>20</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>0.570846</td>\n",
       "      <td>0.518810</td>\n",
       "      <td>0.311401</td>\n",
       "      <td>0.659562</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>0.247094</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>30</td>\n",
       "      <td>0.361364</td>\n",
       "      <td>0.590575</td>\n",
       "      <td>0.524048</td>\n",
       "      <td>0.316268</td>\n",
       "      <td>0.685585</td>\n",
       "      <td>0.361364</td>\n",
       "      <td>0.252415</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>40</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>0.559569</td>\n",
       "      <td>0.516905</td>\n",
       "      <td>0.313103</td>\n",
       "      <td>0.644490</td>\n",
       "      <td>0.356818</td>\n",
       "      <td>0.250090</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.541234</td>\n",
       "      <td>0.518810</td>\n",
       "      <td>0.347072</td>\n",
       "      <td>0.619004</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.297967</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.388636</td>\n",
       "      <td>0.537472</td>\n",
       "      <td>0.521190</td>\n",
       "      <td>0.369322</td>\n",
       "      <td>0.613312</td>\n",
       "      <td>0.388636</td>\n",
       "      <td>0.329188</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>40</td>\n",
       "      <td>0.377273</td>\n",
       "      <td>0.538660</td>\n",
       "      <td>0.518571</td>\n",
       "      <td>0.351326</td>\n",
       "      <td>0.615511</td>\n",
       "      <td>0.377273</td>\n",
       "      <td>0.304149</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.411364</td>\n",
       "      <td>0.533538</td>\n",
       "      <td>0.524524</td>\n",
       "      <td>0.402697</td>\n",
       "      <td>0.607027</td>\n",
       "      <td>0.411364</td>\n",
       "      <td>0.376534</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.15</td>\n",
       "      <td>30</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.539365</td>\n",
       "      <td>0.529524</td>\n",
       "      <td>0.410570</td>\n",
       "      <td>0.613939</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.386214</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15</td>\n",
       "      <td>40</td>\n",
       "      <td>0.402273</td>\n",
       "      <td>0.525311</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.392070</td>\n",
       "      <td>0.597159</td>\n",
       "      <td>0.402273</td>\n",
       "      <td>0.363432</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Contamination Values  Bin Values  Accuracy  Precision (Macro)  \\\n",
       "0                  0.05          20  0.356818           0.570846   \n",
       "1                  0.05          30  0.361364           0.590575   \n",
       "2                  0.05          40  0.356818           0.559569   \n",
       "3                  0.10          20  0.375000           0.541234   \n",
       "4                  0.10          30  0.388636           0.537472   \n",
       "5                  0.10          40  0.377273           0.538660   \n",
       "6                  0.15          20  0.411364           0.533538   \n",
       "7                  0.15          30  0.418182           0.539365   \n",
       "8                  0.15          40  0.402273           0.525311   \n",
       "\n",
       "   Recall (Macro)  F1 Score (Macro)  Precision (Weighted)  Recall (Weighted)  \\\n",
       "0        0.518810          0.311401              0.659562           0.356818   \n",
       "1        0.524048          0.316268              0.685585           0.361364   \n",
       "2        0.516905          0.313103              0.644490           0.356818   \n",
       "3        0.518810          0.347072              0.619004           0.375000   \n",
       "4        0.521190          0.369322              0.613312           0.388636   \n",
       "5        0.518571          0.351326              0.615511           0.377273   \n",
       "6        0.524524          0.402697              0.607027           0.411364   \n",
       "7        0.529524          0.410570              0.613939           0.418182   \n",
       "8        0.517857          0.392070              0.597159           0.402273   \n",
       "\n",
       "   F1 Score (Weighted)  ROC-AUC  \n",
       "0             0.247094      0.5  \n",
       "1             0.252415      0.5  \n",
       "2             0.250090      0.5  \n",
       "3             0.297967      0.5  \n",
       "4             0.329188      0.5  \n",
       "5             0.304149      0.5  \n",
       "6             0.376534      0.5  \n",
       "7             0.386214      0.5  \n",
       "8             0.363432      0.5  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.hbos import HBOS\n",
    "\n",
    "# Parameters to try\n",
    "contamination_values = [0.05, 0.1, 0.15]\n",
    "num_bins_values = [20, 30, 40]  \n",
    "\n",
    "results_hb = []\n",
    "\n",
    "for contamination in contamination_values:\n",
    "    for num_bins in num_bins_values:\n",
    "        hbos_model = HBOS(contamination=contamination, n_bins=num_bins)\n",
    "        hbos_model.fit(X_train)\n",
    "        preds_hb = hbos_model.predict(X_test_both)\n",
    "\n",
    "        print(f\"Contamination: {contamination}, Bins: {num_bins}\")\n",
    "        print(\"Precision:\", precision_score(y_test_both, preds_hb, average = \"weighted\"))\n",
    "        print(\"Recall:\", recall_score(y_test_both, preds_hb, average = \"weighted\"))\n",
    "        print(\"F1 Score:\", f1_score(y_test_both, preds_hb, average = \"weighted\"))\n",
    "        print()\n",
    "\n",
    "        # Classification Report\n",
    "        report = classification_report(y_test_both, preds_hb, target_names=['Good', 'Bad'], output_dict=True)\n",
    "    \n",
    "        resulthb_dict = {\n",
    "            \"Contamination Values\": contamination,\n",
    "            \"Bin Values\": num_bins,\n",
    "            \"Accuracy\": report['accuracy'],\n",
    "            \"Precision (Macro)\": report['macro avg']['precision'],\n",
    "            \"Recall (Macro)\": report['macro avg']['recall'],\n",
    "            \"F1 Score (Macro)\": report['macro avg']['f1-score'],\n",
    "            \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
    "            \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
    "            \"F1 Score (Weighted)\": report['weighted avg']['f1-score'],\n",
    "            \"ROC-AUC\": roc_auc_score(y_test_both, preds)\n",
    "        }\n",
    "\n",
    "        results_hb.append(resulthb_dict)\n",
    "\n",
    "# Create a DataFrame\n",
    "results_hbos = pd.DataFrame(results_hb)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_hbos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HBOS model, accuracy, weighted recall and F-1 scores increased higher bin and contamination values. Highest precision (weighted) is 0.685 with 0.05 contamination and 30 bins. ROC-AUC scores are consistently around 0.5, indicating that the models are not effectively distinguishing between positive and negative instances\n",
    "\n",
    "Both models show a trade-off between precision and recall. The performance of both models depends on the choice of hyperparameters, specifically contamination values. As contamination values increase, the models become more sensitive to anomalies but might generate more false positives. COPOD generally outperformed HBOS in terms of precision and ROC-AUC scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges Faced: ABOD Model\n",
    "\n",
    "While exploring different anomaly detection models, attempts were made to implement the Angle-Based Outlier Detection (ABOD) model. However, it is noteworthy that the ABOD model encountered technical issues and did not run successfully on the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continual Learning Approach\n",
    "\n",
    "The purpose of continual learning is to contuniously adapt to new challenges in dynamic environments while retaining past knowledge. It is important to model the normal behavior and identify anomalies as observations that differ from the modeled behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenarios for Continual Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining data is divided into 'normal' and 'anomaly' subsets based on the binary 'anomaly' column which includes \"good\" and \"bad\" credit risks. K-Means clustering is separately applied to both the 'normal' and 'anomaly' subsets, creating clusters for each. The function iterates over the clusters, creating training and evaluation batches for the 'normal' subset. For the 'anomaly' subset, anomalies are only included in the evaluation batches. This process results in distinct clusters for normal and anomaly instances, facilitating the creation of batches for continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "####################################################################################\n",
    "def clustering(filename, train_eval_perc, n_clusters, dataset_name, drop_attrs):\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    for a in drop_attrs:\n",
    "        data.drop(columns=a, inplace=True)\n",
    "\n",
    "    print(data)\n",
    "    print(data.head)\n",
    "    normal = data[data[\"anomaly\"] == 0]\n",
    "    print(f'Normal {len(normal)}')\n",
    "\n",
    "    anomalies = data[data[\"anomaly\"] == 1]\n",
    "    print(f'Anomalies {len(anomalies)}')\n",
    "\n",
    "    normal = clean_dataset(normal)\n",
    "    anomalies = clean_dataset(anomalies)\n",
    "\n",
    "    clust = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "    normal_clustering = clust.fit(normal)\n",
    "    normal_ids = normal_clustering.predict(normal)\n",
    "    print(f'{len(set(normal_ids))} clusters extracted for Normal')\n",
    "    print(normal_ids)\n",
    "\n",
    "    anomalies_clustering = clust.fit(anomalies)\n",
    "    anomalies_ids = anomalies_clustering.predict(anomalies)\n",
    "    print(f'{len(set(anomalies_ids))} clusters extracted for Anomalies')\n",
    "    print(anomalies_ids)\n",
    "\n",
    "    # TODO: closest, random : add param\n",
    "\n",
    "    for c in range(len(set(normal_ids))):\n",
    "        print(f'Cluster {c}')\n",
    "        normal_idx = [i for i in range(len(normal_ids)) if normal_ids[i] == c]\n",
    "        normal_data_c = normal.iloc[normal_idx]\n",
    "        print(len(normal_data_c))\n",
    "\n",
    "        # Divide normal in training and evaluation batches\n",
    "        normal_train, normal_eval = divide(normal_data_c, train_eval_perc)\n",
    "        print(f'Training: {train_eval_perc}% - {len(normal_train)} - Evaluation: {100-train_eval_perc}% - {len(normal_eval)}')\n",
    "        normal_train.to_csv(f'{dataset_name}_{c}_train.csv')\n",
    "\n",
    "        # Anomalies are only in evaluation batches\n",
    "        anomalies_idx = [i for i in range(len(anomalies_ids)) if anomalies_ids[i] == c]\n",
    "        anomalies_data_c = anomalies.iloc[anomalies_idx]\n",
    "        print(f'Anomalies: {len(anomalies_data_c)}')\n",
    "\n",
    "        eval_batch_c = pd.concat([normal_eval, anomalies_data_c])\n",
    "        print(f'Evaluation batch: {len(eval_batch_c)}')\n",
    "        eval_batch_c.to_csv(f'{dataset_name}_{c}_eval.csv')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "def divide(cluster_data, perc):\n",
    "    num_rows = cluster_data.shape[0]\n",
    "    num_rows_p1 = int(np.rint((num_rows / 100) * perc))\n",
    "\n",
    "    part_1 = cluster_data.iloc[0:num_rows_p1]\n",
    "    part_2 = cluster_data.iloc[num_rows_p1 + 1:]\n",
    "\n",
    "    return part_1, part_2\n",
    "\n",
    "####################################################################################\n",
    "def reduce_anomalies(filename, dataset_name, num_tasks, anomaly_ratio):\n",
    "\n",
    "    for i in range(num_tasks): # Open files containing anomalies (eval batches)\n",
    "        eval_b = pd.read_csv(f'{dataset_name}_{i}_eval.csv')\n",
    "        eval_labels = eval_b[\"anomaly\"]\n",
    "\n",
    "        normal = eval_b[eval_b[\"anomaly\"] == 0]\n",
    "        print(f'Normal {len(normal)}')\n",
    "\n",
    "        anomalies = eval_b[eval_b[\"anomaly\"] == 1]\n",
    "        print(f'Anomalies {len(anomalies)}')\n",
    "\n",
    "        total_size = len(normal) + len(anomalies)\n",
    "        curr_anom_ratio = len(anomalies) / total_size\n",
    "        print(f'Curr anom ratio {curr_anom_ratio}')\n",
    "\n",
    "        #num_to_sample = anomaly_ratio * total_size\n",
    "        # rand_indices = np.random.choice(range(len(anomalies)), num_to_sample, replace=False)\n",
    "        # sampled_anomalies = anomalies.iloc[rand_indices]\n",
    "        sampled_anomalies = anomalies.sample(frac=anomaly_ratio)\n",
    "\n",
    "        new_batch = pd.concat([normal, sampled_anomalies])\n",
    "        new_batch.to_csv(f'{dataset_name}-r-{anomaly_ratio}_{i}_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset with the updated labels\n",
    "ccard.to_csv(\"german_creditcard1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1  2   3  4  5  6  7  8   9  ...  15  16  17  18  19  20  21  22  23  \\\n",
       "0    1   6  4  12  5  5  3  4  1  67  ...   0   0   1   0   0   1   0   0   1   \n",
       "1    2  48  2  60  1  3  2  2  1  22  ...   0   0   1   0   0   1   0   0   1   \n",
       "2    4  12  4  21  1  4  3  3  1  49  ...   0   0   1   0   0   1   0   1   0   \n",
       "3    1  42  2  79  1  4  3  4  2  45  ...   0   0   0   0   0   0   0   0   1   \n",
       "4    1  24  3  49  1  3  3  4  4  53  ...   1   0   1   0   0   0   0   0   1   \n",
       "..  ..  .. ..  .. .. .. .. .. ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "995  4  12  2  17  1  4  2  4  1  31  ...   0   0   1   0   0   1   0   1   0   \n",
       "996  1  30  2  39  1  3  1  4  2  40  ...   0   1   1   0   0   1   0   0   0   \n",
       "997  4  12  2   8  1  5  3  4  3  38  ...   0   0   1   0   0   1   0   0   1   \n",
       "998  1  45  2  18  1  3  3  4  4  23  ...   0   0   1   0   0   0   0   0   1   \n",
       "999  2  45  4  46  2  1  3  4  3  27  ...   0   1   1   0   0   1   0   0   1   \n",
       "\n",
       "     anomaly  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "..       ...  \n",
       "995        0  \n",
       "996        0  \n",
       "997        0  \n",
       "998        1  \n",
       "999        0  \n",
       "\n",
       "[1000 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"german_creditcard1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1  2   3  4  5  6  7  8   9  ...  15  16  17  18  19  20  21  22  23  \\\n",
      "0    1   6  4  12  5  5  3  4  1  67  ...   0   0   1   0   0   1   0   0   1   \n",
      "1    2  48  2  60  1  3  2  2  1  22  ...   0   0   1   0   0   1   0   0   1   \n",
      "2    4  12  4  21  1  4  3  3  1  49  ...   0   0   1   0   0   1   0   1   0   \n",
      "3    1  42  2  79  1  4  3  4  2  45  ...   0   0   0   0   0   0   0   0   1   \n",
      "4    1  24  3  49  1  3  3  4  4  53  ...   1   0   1   0   0   0   0   0   1   \n",
      "..  ..  .. ..  .. .. .. .. .. ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "995  4  12  2  17  1  4  2  4  1  31  ...   0   0   1   0   0   1   0   1   0   \n",
      "996  1  30  2  39  1  3  1  4  2  40  ...   0   1   1   0   0   1   0   0   0   \n",
      "997  4  12  2   8  1  5  3  4  3  38  ...   0   0   1   0   0   1   0   0   1   \n",
      "998  1  45  2  18  1  3  3  4  4  23  ...   0   0   1   0   0   0   0   0   1   \n",
      "999  2  45  4  46  2  1  3  4  3  27  ...   0   1   1   0   0   1   0   0   1   \n",
      "\n",
      "     anomaly  \n",
      "0          0  \n",
      "1          1  \n",
      "2          0  \n",
      "3          0  \n",
      "4          1  \n",
      "..       ...  \n",
      "995        0  \n",
      "996        0  \n",
      "997        0  \n",
      "998        1  \n",
      "999        0  \n",
      "\n",
      "[1000 rows x 25 columns]\n",
      "<bound method NDFrame.head of      0   1  2   3  4  5  6  7  8   9  ...  15  16  17  18  19  20  21  22  23  \\\n",
      "0    1   6  4  12  5  5  3  4  1  67  ...   0   0   1   0   0   1   0   0   1   \n",
      "1    2  48  2  60  1  3  2  2  1  22  ...   0   0   1   0   0   1   0   0   1   \n",
      "2    4  12  4  21  1  4  3  3  1  49  ...   0   0   1   0   0   1   0   1   0   \n",
      "3    1  42  2  79  1  4  3  4  2  45  ...   0   0   0   0   0   0   0   0   1   \n",
      "4    1  24  3  49  1  3  3  4  4  53  ...   1   0   1   0   0   0   0   0   1   \n",
      "..  ..  .. ..  .. .. .. .. .. ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "995  4  12  2  17  1  4  2  4  1  31  ...   0   0   1   0   0   1   0   1   0   \n",
      "996  1  30  2  39  1  3  1  4  2  40  ...   0   1   1   0   0   1   0   0   0   \n",
      "997  4  12  2   8  1  5  3  4  3  38  ...   0   0   1   0   0   1   0   0   1   \n",
      "998  1  45  2  18  1  3  3  4  4  23  ...   0   0   1   0   0   0   0   0   1   \n",
      "999  2  45  4  46  2  1  3  4  3  27  ...   0   1   1   0   0   1   0   0   1   \n",
      "\n",
      "     anomaly  \n",
      "0          0  \n",
      "1          1  \n",
      "2          0  \n",
      "3          0  \n",
      "4          1  \n",
      "..       ...  \n",
      "995        0  \n",
      "996        0  \n",
      "997        0  \n",
      "998        1  \n",
      "999        0  \n",
      "\n",
      "[1000 rows x 25 columns]>\n",
      "Normal 700\n",
      "Anomalies 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-79204432c8a0>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-6-79204432c8a0>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 clusters extracted for Normal\n",
      "[1 1 3 2 1 3 1 4 4 1 3 0 1 1 1 1 4 4 4 4 4 4 0 3 1 4 3 4 4 0 4 3 3 4 0 4 3\n",
      " 4 0 3 4 0 4 2 4 4 1 0 3 4 4 0 3 1 1 3 1 0 2 0 1 4 1 1 4 4 4 1 1 0 1 1 0 0\n",
      " 3 4 0 4 4 4 3 3 4 4 4 3 4 0 4 4 0 4 1 4 4 4 2 0 1 2 4 3 0 1 4 3 0 4 0 4 1\n",
      " 3 1 4 4 3 3 3 1 1 0 4 0 4 4 3 4 4 4 0 4 4 1 1 4 1 0 4 1 0 4 4 0 1 4 3 4 2\n",
      " 1 4 3 1 0 0 0 4 0 0 0 1 4 4 4 0 0 0 0 1 4 4 4 4 0 1 4 0 4 1 4 4 4 1 0 4 3\n",
      " 0 0 1 4 1 3 1 4 3 4 4 0 4 2 4 4 1 4 0 1 1 1 0 2 3 3 1 4 3 3 3 0 1 1 0 1 1\n",
      " 4 0 4 3 3 0 4 4 0 4 0 0 0 4 0 3 4 0 4 3 4 0 4 3 0 0 0 0 1 4 0 4 4 0 1 1 4\n",
      " 1 0 4 4 4 0 0 0 0 3 4 2 4 4 1 4 4 4 0 4 4 3 4 4 4 4 0 4 4 2 1 0 1 4 4 4 0\n",
      " 1 4 0 4 3 4 3 3 0 0 4 1 4 0 0 1 4 1 4 0 4 4 4 1 4 4 0 4 4 1 2 4 0 0 0 0 4\n",
      " 0 0 4 0 4 4 1 3 0 0 4 0 3 4 1 0 4 1 4 1 1 1 4 0 4 4 4 4 4 4 4 3 1 4 4 2 0\n",
      " 0 4 1 4 4 1 4 3 0 4 3 4 4 4 0 4 4 0 1 0 0 1 1 4 4 2 4 1 4 1 1 4 0 4 0 0 3\n",
      " 3 4 4 0 0 4 4 1 4 1 4 4 1 1 4 0 1 4 0 1 4 1 0 0 2 2 0 1 0 4 4 4 0 1 0 4 1\n",
      " 4 2 4 4 4 4 0 3 0 0 2 0 3 4 1 4 1 3 0 0 0 0 3 2 4 0 3 1 4 1 4 4 3 2 3 4 0\n",
      " 4 0 4 4 0 0 0 1 4 4 4 4 1 0 0 3 4 4 4 1 4 3 0 4 0 3 1 4 1 4 3 1 0 0 0 0 3\n",
      " 0 4 0 0 2 4 4 3 0 1 4 3 1 4 3 4 0 4 0 0 1 0 0 1 1 0 0 3 1 1 4 3 4 0 0 3 4\n",
      " 1 0 4 4 1 4 1 1 0 4 3 4 1 2 4 4 3 1 4 2 4 0 1 0 4 3 1 3 4 4 1 1 0 1 0 0 4\n",
      " 1 0 4 2 4 4 0 0 3 0 4 4 0 3 4 3 1 4 4 0 4 4 0 3 3 2 0 1 0 3 3 0 1 4 3 4 2\n",
      " 0 4 4 0 2 4 4 4 0 0 0 0 0 0 4 4 4 2 4 4 1 1 4 4 4 4 4 3 0 4 4 4 1 2 0 0 4\n",
      " 1 0 4 0 4 1 0 4 4 1 3 0 4 3 0 1 1 1 0 4 4 3 1 3 1 0 4 4 0 1 4 0 4 0]\n",
      "5 clusters extracted for Anomalies\n",
      "[0 3 3 1 3 4 1 2 0 3 1 0 4 0 0 4 2 3 3 3 0 2 1 1 2 2 0 0 0 3 1 1 1 3 0 4 1\n",
      " 1 1 1 1 1 3 0 0 3 1 1 4 1 3 3 3 3 1 1 3 1 3 4 2 0 1 3 1 2 4 1 3 1 3 1 1 0\n",
      " 3 2 3 4 0 0 3 1 0 1 1 1 3 3 1 1 0 2 3 4 1 1 1 0 1 3 3 1 3 2 0 2 2 4 0 1 1\n",
      " 4 1 1 1 1 4 2 1 1 1 3 3 4 3 3 1 3 1 1 1 3 4 0 3 0 3 1 1 1 0 1 0 3 0 3 1 1\n",
      " 0 1 0 3 4 1 0 1 1 3 3 1 1 2 1 0 3 3 1 3 1 4 4 3 1 1 1 3 1 1 1 3 4 1 4 1 3\n",
      " 1 3 4 1 4 1 1 3 1 3 0 1 1 1 1 4 0 1 1 3 4 0 1 0 0 1 1 2 1 1 1 3 1 0 1 2 3\n",
      " 1 1 4 1 1 1 1 2 3 0 1 3 1 0 0 1 0 0 1 0 3 3 1 3 3 3 0 1 2 1 4 3 1 0 4 1 1\n",
      " 3 1 1 1 1 3 4 2 3 4 3 3 2 2 1 3 1 0 4 0 1 1 1 0 3 4 3 3 3 2 3 1 3 1 0 4 1\n",
      " 0 3 0 3]\n",
      "Cluster 0\n",
      "182\n",
      "Training: 70% - 127 - Evaluation: 30% - 54\n",
      "Anomalies: 50\n",
      "Evaluation batch: 104\n",
      "Cluster 1\n",
      "134\n",
      "Training: 70% - 94 - Evaluation: 30% - 39\n",
      "Anomalies: 120\n",
      "Evaluation batch: 159\n",
      "Cluster 2\n",
      "28\n",
      "Training: 70% - 20 - Evaluation: 30% - 7\n",
      "Anomalies: 22\n",
      "Evaluation batch: 29\n",
      "Cluster 3\n",
      "85\n",
      "Training: 70% - 60 - Evaluation: 30% - 24\n",
      "Anomalies: 77\n",
      "Evaluation batch: 101\n",
      "Cluster 4\n",
      "271\n",
      "Training: 70% - 190 - Evaluation: 30% - 80\n",
      "Anomalies: 31\n",
      "Evaluation batch: 111\n"
     ]
    }
   ],
   "source": [
    "filename = \"german_creditcard1.csv\"\n",
    "dataset_name = \"ccard\"\n",
    "drop_attrs = []\n",
    "\n",
    "\n",
    "n_clusters = 5\n",
    "train_eval_perc = 70\n",
    "\n",
    "clustering(filename, train_eval_perc, n_clusters,  dataset_name, drop_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import SpectralClustering, OPTICS\n",
    "from utils import *\n",
    "import pickle\n",
    "import csv\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of continual learning scenario for anomaly detection is presented below, employing models like Local Outlier Factor and Isolation Forest. It sequentially trains these models on tasks and evaluates their performance using ROC-AUC scores. Different strategies, such as \"Naive\", which forgets the previous task and considered as the baseline, \"Replay\", which is the summarized version of the task, and \"Cumulative\", which uses all the tasks with low forgetting and slow training, are explored. The evaluation includes metrics like Lifelong ROC (LROC), Backward Transfer (BWT), and Forward Transfer (FWT). Results are logged, and heatmaps illustrating ROC-AUC scores are generated for analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Naive\n",
      "__________________\n",
      "LocalOutlierFactor(n_neighbors=10, novelty=True)\n",
      "__________________\n",
      "\n",
      "Current task: 0\n",
      "Train 127\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.7970370370370371\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.864957264957265\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8376623376623378\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.821969696969697\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.769758064516129\n",
      "Current task: 1\n",
      "Train 94\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8096296296296297\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8762820512820512\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8116883116883117\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8214285714285715\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.8419354838709677\n",
      "Current task: 2\n",
      "Train 20\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8062962962962963\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8476495726495726\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8441558441558441\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.7738095238095237\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.7870967741935483\n",
      "Current task: 3\n",
      "Train 60\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.7855555555555556\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.7461538461538462\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.7857142857142856\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.739177489177489\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.6973790322580645\n",
      "Current task: 4\n",
      "Train 190\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.7837037037037037\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8803418803418803\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.7987012987012988\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8225108225108225\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.816532258064516\n",
      "[[0.8  0.86 0.84 0.82 0.77]\n",
      " [0.81 0.88 0.81 0.82 0.84]\n",
      " [0.81 0.85 0.84 0.77 0.79]\n",
      " [0.79 0.75 0.79 0.74 0.7 ]\n",
      " [0.78 0.88 0.8  0.82 0.82]]\n",
      "strategy,params,model,LROC,BWT,FWT\n",
      "Naive,,LocalOutlierFactor(n_neighbors=10, novelty=True),0.8106666666666666,-0.017999999999999995,0.8019999999999999\n",
      "__________________\n",
      "IsolationForest(random_state=0)\n",
      "__________________\n",
      "\n",
      "Current task: 0\n",
      "Train 127\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3666666666666667\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.5878205128205128\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4285714285714286\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5524891774891775\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.3060483870967741\n",
      "Current task: 1\n",
      "Train 94\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.33444444444444443\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.36047008547008547\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.5844155844155844\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5097402597402598\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.42459677419354835\n",
      "Current task: 2\n",
      "Train 20\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.44148148148148153\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.40405982905982907\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4220779220779221\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.3111471861471861\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.3435483870967741\n",
      "Current task: 3\n",
      "Train 60\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.5111111111111111\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.5051282051282051\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4935064935064935\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.46645021645021645\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.43266129032258066\n",
      "Current task: 4\n",
      "Train 190\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3662962962962963\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.605982905982906\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.5909090909090908\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5833333333333334\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.25483870967741934\n",
      "[[0.37 0.59 0.43 0.55 0.31]\n",
      " [0.33 0.36 0.58 0.51 0.42]\n",
      " [0.44 0.4  0.42 0.31 0.34]\n",
      " [0.51 0.51 0.49 0.47 0.43]\n",
      " [0.37 0.61 0.59 0.58 0.25]]\n",
      "strategy,params,model,LROC,BWT,FWT\n",
      "Naive,,IsolationForest(random_state=0),0.44666666666666666,0.096,0.44699999999999995\n",
      "['strategy,params,model,LROC,BWT,FWT', 'Naive,,LocalOutlierFactor(n_neighbors=10_novelty=True),0.8106666666666666,-0.017999999999999995,0.8019999999999999', 'Naive,,IsolationForest(random_state=0),0.44666666666666666,0.096,0.44699999999999995']\n",
      "[[0.37 0.59 0.43 0.55 0.31]\n",
      " [0.33 0.36 0.58 0.51 0.42]\n",
      " [0.44 0.4  0.42 0.31 0.34]\n",
      " [0.51 0.51 0.49 0.47 0.43]\n",
      " [0.37 0.61 0.59 0.58 0.25]]\n",
      "Strategy: Replay\n",
      "__________________\n",
      "LocalOutlierFactor(n_neighbors=10, novelty=True)\n",
      "__________________\n",
      "\n",
      "Current task: 0\n",
      "Train 127\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.7970370370370371\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.864957264957265\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8376623376623378\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.821969696969697\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.769758064516129\n",
      "Current task: 1\n",
      "Train 44\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8437037037037037\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8771367521367521\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8376623376623378\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8208874458874459\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.8225806451612904\n",
      "Current task: 2\n",
      "Train 48\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8455555555555556\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8764957264957265\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8441558441558442\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.814935064935065\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.8201612903225807\n",
      "Current task: 3\n",
      "Train 60\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8485185185185186\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8741452991452991\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8506493506493507\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8198051948051948\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.815725806451613\n",
      "Current task: 4\n",
      "Train 98\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8377777777777778\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8777777777777778\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8376623376623378\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8203463203463204\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.8286290322580645\n",
      "[[0.8  0.86 0.84 0.82 0.77]\n",
      " [0.84 0.88 0.84 0.82 0.82]\n",
      " [0.85 0.88 0.84 0.81 0.82]\n",
      " [0.85 0.87 0.85 0.82 0.82]\n",
      " [0.84 0.88 0.84 0.82 0.83]]\n",
      "strategy,params,model,LROC,BWT,FWT\n",
      "Replay,budget=0.2_compact_True,LocalOutlierFactor(n_neighbors=10, novelty=True),0.846,0.01799999999999997,0.8220000000000001\n",
      "__________________\n",
      "IsolationForest(random_state=0)\n",
      "__________________\n",
      "\n",
      "Current task: 0\n",
      "Train 127\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3666666666666667\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.5878205128205128\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4285714285714286\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5524891774891775\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.3060483870967741\n",
      "Current task: 1\n",
      "Train 44\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.35962962962962963\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.4861111111111111\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.5064935064935064\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5589826839826839\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.39475806451612905\n",
      "Current task: 2\n",
      "Train 48\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3281481481481482\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.49743589743589745\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4025974025974026\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.6152597402597403\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.38185483870967746\n",
      "Current task: 3\n",
      "Train 60\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.38962962962962966\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.5076923076923077\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.5194805194805195\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5551948051948052\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.44516129032258067\n",
      "Current task: 4\n",
      "Train 98\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3351851851851852\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.504059829059829\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.44155844155844154\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5930735930735931\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.3455645161290322\n",
      "[[0.37 0.59 0.43 0.55 0.31]\n",
      " [0.36 0.49 0.51 0.56 0.39]\n",
      " [0.33 0.5  0.4  0.62 0.38]\n",
      " [0.39 0.51 0.52 0.56 0.45]\n",
      " [0.34 0.5  0.44 0.59 0.35]]\n",
      "strategy,params,model,LROC,BWT,FWT\n",
      "Replay,budget=0.2_compact_True,IsolationForest(random_state=0),0.4433333333333333,0.016999999999999998,0.4789999999999999\n",
      "['strategy,params,model,LROC,BWT,FWT', 'Replay,budget=0.2_compact_True,LocalOutlierFactor(n_neighbors=10_novelty=True),0.846,0.01799999999999997,0.8220000000000001', 'Replay,budget=0.2_compact_True,IsolationForest(random_state=0),0.4433333333333333,0.016999999999999998,0.4789999999999999']\n",
      "[[0.37 0.59 0.43 0.55 0.31]\n",
      " [0.36 0.49 0.51 0.56 0.39]\n",
      " [0.33 0.5  0.4  0.62 0.38]\n",
      " [0.39 0.51 0.52 0.56 0.45]\n",
      " [0.34 0.5  0.44 0.59 0.35]]\n",
      "Strategy: Cumulative\n",
      "__________________\n",
      "LocalOutlierFactor(n_neighbors=10, novelty=True)\n",
      "__________________\n",
      "\n",
      "Current task: 0\n",
      "Train 127\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.7970370370370371\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.864957264957265\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8376623376623378\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.821969696969697\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.769758064516129\n",
      "Current task: 1\n",
      "Train 221\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.77\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8707264957264957\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8376623376623378\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8235930735930737\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.7919354838709678\n",
      "Current task: 2\n",
      "Train 241\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.7988888888888889\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8711538461538462\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8441558441558441\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8187229437229437\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.790725806451613\n",
      "Current task: 3\n",
      "Train 301\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.832962962962963\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8730769230769231\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8506493506493507\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8187229437229437\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.8020161290322582\n",
      "Current task: 4\n",
      "Train 491\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.8292592592592594\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.8782051282051282\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.8571428571428571\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.8181818181818181\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.7616935483870967\n",
      "[[0.8  0.86 0.84 0.82 0.77]\n",
      " [0.77 0.87 0.84 0.82 0.79]\n",
      " [0.8  0.87 0.84 0.82 0.79]\n",
      " [0.83 0.87 0.85 0.82 0.8 ]\n",
      " [0.83 0.88 0.86 0.82 0.76]]\n",
      "strategy,params,model,LROC,BWT,FWT\n",
      "Cumulative,,LocalOutlierFactor(n_neighbors=10, novelty=True),0.8313333333333333,0.006999999999999984,0.8149999999999998\n",
      "__________________\n",
      "IsolationForest(random_state=0)\n",
      "__________________\n",
      "\n",
      "Current task: 0\n",
      "Train 127\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3666666666666667\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.5878205128205128\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4285714285714286\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5524891774891775\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.3060483870967741\n",
      "Current task: 1\n",
      "Train 221\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3759259259259259\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.42243589743589743\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4675324675324676\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5589826839826839\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.4016129032258064\n",
      "Current task: 2\n",
      "Train 241\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3733333333333333\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.47670940170940174\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4285714285714286\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5524891774891775\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.39274193548387093\n",
      "Current task: 3\n",
      "Train 301\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.34629629629629627\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.4408119658119658\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.36363636363636365\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.501082251082251\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.37459677419354837\n",
      "Current task: 4\n",
      "Train 491\n",
      "Eval 0 - Len: 104\n",
      "50/104 anomalies\n",
      "ROC: 0.3144444444444444\n",
      "Eval 1 - Len: 159\n",
      "120/159 anomalies\n",
      "ROC: 0.5194444444444445\n",
      "Eval 2 - Len: 29\n",
      "22/29 anomalies\n",
      "ROC: 0.4610389610389611\n",
      "Eval 3 - Len: 101\n",
      "77/101 anomalies\n",
      "ROC: 0.5622294372294372\n",
      "Eval 4 - Len: 111\n",
      "31/111 anomalies\n",
      "ROC: 0.3048387096774194\n",
      "[[0.37 0.59 0.43 0.55 0.31]\n",
      " [0.38 0.42 0.47 0.56 0.4 ]\n",
      " [0.37 0.48 0.43 0.55 0.39]\n",
      " [0.35 0.44 0.36 0.5  0.37]\n",
      " [0.31 0.52 0.46 0.56 0.3 ]]\n",
      "strategy,params,model,LROC,BWT,FWT\n",
      "Cumulative,,IsolationForest(random_state=0),0.4166666666666667,0.013000000000000012,0.4620000000000001\n",
      "['strategy,params,model,LROC,BWT,FWT', 'Cumulative,,LocalOutlierFactor(n_neighbors=10_novelty=True),0.8313333333333333,0.006999999999999984,0.8149999999999998', 'Cumulative,,IsolationForest(random_state=0),0.4166666666666667,0.013000000000000012,0.4620000000000001']\n",
      "[[0.37 0.59 0.43 0.55 0.31]\n",
      " [0.38 0.42 0.47 0.56 0.4 ]\n",
      " [0.37 0.48 0.43 0.55 0.39]\n",
      " [0.35 0.44 0.36 0.5  0.37]\n",
      " [0.31 0.52 0.46 0.56 0.3 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_tasks = 5\n",
    "\n",
    "# dataset_name = \"pv-italy\"\n",
    "# drop_cols = [\"anomaly\", \"id\"]\n",
    "\n",
    "eval_standard_metrics = False\n",
    "dataset_name = \"ccard\"\n",
    "drop_cols = [\"anomaly\"]\n",
    "\n",
    "models = [LocalOutlierFactor(n_neighbors=10, novelty=True),\n",
    "          #LocalOutlierFactor(n_neighbors=25, novelty=True),\n",
    "          #LocalOutlierFactor(n_neighbors=50, novelty=True),\n",
    "          #LocalOutlierFactor(n_neighbors=100, novelty=True),\n",
    "            IsolationForest(random_state=0),\n",
    "        #    ABOD(contamination=0.01),\n",
    "        #    HBOS(contamination=0.01)\n",
    "          #AutoEncoder(hidden_neurons=[20, 10, 5, 10, 20], epochs=20, contamination=0.05, batch_size=64, verbose=True)\n",
    "          # OneClassSVM(gamma='auto', kernel='rbf'),\n",
    "          # OneClassSVM(gamma='auto', kernel='poly'),\n",
    "          # OneClassSVM(gamma='auto', kernel='sigmoid'),\n",
    "          # OneClassSVM(gamma='auto', kernel='linear')\n",
    "          ]\n",
    "\n",
    "#strategy = 'Replay Graph'\n",
    "rep_budget = 0.2\n",
    "rep_compact = True      # Use a sample from last task instead of full data\n",
    "const_ratio_replay = False\n",
    "min_samples_cluster = False\n",
    "skip_noise = False\n",
    "\n",
    "strategies = [\"Naive\", \"Replay\", \"Cumulative\"]\n",
    "#strategies = [\"MSTE\"]\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"Naive\": \"\",\n",
    "    \"Replay\": f'budget={rep_budget}_compact_{rep_compact}',\n",
    "    \"Replay Graph\": f'budget={rep_budget}_compact_{rep_compact}_const_ratio_replay_{const_ratio_replay}_min_samples_cluster_{min_samples_cluster}_skip_noise_{skip_noise}',\n",
    "    \"Cumulative\": \"\",\n",
    "    \"MSTE\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "# params = ''\n",
    "\n",
    "\n",
    "tasks_train = []\n",
    "tasks_eval = []\n",
    "labels_eval = []\n",
    "\n",
    "MSTE = []\n",
    "\n",
    "# Load data\n",
    "for i in range(num_tasks):\n",
    "    train_b = pd.read_csv(f'{dataset_name}_{i}_train.csv')\n",
    "    train_b.drop(columns=drop_cols, inplace=True)   # Remove class attribute from training sets\n",
    "    tasks_train.append(train_b)\n",
    "\n",
    "    eval_b = pd.read_csv(f'{dataset_name}_{i}_eval.csv')\n",
    "    eval_labels = eval_b[\"anomaly\"]\n",
    "    eval_b.drop(columns=drop_cols, inplace=True)\n",
    "    tasks_eval.append(eval_b)\n",
    "    labels_eval.append(eval_labels)\n",
    "\n",
    "# Run scenario (train and predict)\n",
    "for s in strategies:\n",
    "    print(f'Strategy: {s}')\n",
    "    logger = []\n",
    "    logger.append('strategy,params,model,LROC,BWT,FWT')\n",
    "\n",
    "    for m in models:\n",
    "        clf = m\n",
    "        mat_f1 = np.zeros((num_tasks, num_tasks))\n",
    "        mat_roc = np.zeros((num_tasks, num_tasks))\n",
    "        print(f'__________________\\n{m}\\n__________________\\n')\n",
    "\n",
    "        for i in range(num_tasks):\n",
    "            print(f'Current task: {i}')\n",
    "            train_b = tasks_train[i]\n",
    "\n",
    "            if s == 'Naive':\n",
    "                print(f'Train {len(train_b)}')\n",
    "                clf = m.fit(train_b)\n",
    "            elif s == 'Cumulative':\n",
    "                if i > 0:\n",
    "                    train_tasks_dfs = []\n",
    "                    for k in range(0, i+1):  # including current task\n",
    "                        train_tasks_dfs.append(tasks_train[k])\n",
    "                    train_b = pd.concat(train_tasks_dfs)\n",
    "                print(f'Train {len(train_b)}')\n",
    "                clf = clf.fit(train_b)\n",
    "            elif s == 'MSTE':\n",
    "                clf = m.fit(train_b)\n",
    "                MSTE.append(clf)\n",
    "            elif s == 'Replay':\n",
    "                if i > 0:\n",
    "                    train_tasks_dfs = []\n",
    "                    for k in range(0, i):\n",
    "                        rb = tasks_train[k].sample(frac=rep_budget)\n",
    "                        train_tasks_dfs.append(rb)\n",
    "                    if rep_compact:\n",
    "                        train_tasks_dfs.append(tasks_train[i].sample(frac=rep_budget))\n",
    "                    else:\n",
    "                        train_tasks_dfs.append(tasks_train[i])\n",
    "                    train_b = pd.concat(train_tasks_dfs)\n",
    "                print(f'Train {len(train_b)}')\n",
    "                clf = clf.fit(train_b)\n",
    "            elif s == 'Replay Graph':    # Just clusters task i-1 and appends it to previous clusters\n",
    "                if i == 0:\n",
    "                    rb_prev_tasks = []\n",
    "                if i > 0:\n",
    "                    train_tasks_dfs = []\n",
    "                    c_sampled_all = []          # Clusters for considered task\n",
    "                    budget = int(rep_budget * len(tasks_train[i-1]))    # Budget for all clusters within a single task\n",
    "                    budget_left = budget\n",
    "                    if min_samples_cluster:\n",
    "                        clustering = OPTICS(min_samples=int(np.sqrt(len(tasks_train[i-1]))/2)).fit_predict(tasks_train[i-1])  # Clusters prev task\n",
    "                    else:\n",
    "                        clustering = OPTICS(min_samples=20).fit_predict(tasks_train[i-1])  # Clusters prev task\n",
    "\n",
    "                    budget_per_cluster = int(budget / len(np.unique(clustering)))\n",
    "\n",
    "                    while budget_left > 0:\n",
    "                        print(f'Budget still available for task: {budget_left}')\n",
    "                        for c in np.unique(clustering):\n",
    "                            if skip_noise:\n",
    "                                if c == -1:\n",
    "                                    budget_per_cluster = int(budget / len(np.unique(clustering) - 1))\n",
    "                                    continue        # Discarding noise cluster\n",
    "                            print(f'Sampling from cluster {c}...')\n",
    "                            c_indices = [i for i in range(len(clustering)) if clustering[i] == c]\n",
    "                            c_data = tasks_train[i-1].iloc[c_indices]\n",
    "                            print(len(c_data))\n",
    "                            if const_ratio_replay:  # vanilla: sample frac of data without caring about num of samples\n",
    "                                c_sampled = c_data.sample(frac=rep_budget)\n",
    "                            else:\n",
    "                                if len(c_data) >= budget_per_cluster:\n",
    "                                    c_sampled = c_data.sample(budget_per_cluster) # chance of smaller cluster than budget\n",
    "                                else:\n",
    "                                    c_sampled = c_data\n",
    "\n",
    "                            budget_left = budget_left - len(c_sampled)\n",
    "                            c_sampled_all.append(c_sampled)\n",
    "\n",
    "                    rb = pd.concat(c_sampled_all)           # Append all clusters for a single task (previous)\n",
    "                    rb_prev_tasks.append(rb)                # Append all clusters for a single task to RB of previous tasks\n",
    "                    rb_all_except_last = pd.concat(rb_prev_tasks)\n",
    "                    print(f'RB all except last size: {np.shape(rb_all_except_last)}')\n",
    "                    if rep_compact:\n",
    "                        train_b = rb_all_except_last.append(tasks_train[i].sample(frac=rep_budget))    # Append sampled current task to clustered prev tasks\n",
    "                    else:\n",
    "                        train_b = rb_all_except_last.append(tasks_train[i])    # Append full current task to clustered prev tasks\n",
    "                    print(f'RB all including last size: {np.shape(train_b)}')\n",
    "                print(f'Train {len(train_b)}')\n",
    "\n",
    "                clf = clf.fit(train_b)\n",
    "            else:\n",
    "                print('Unknown strategy')\n",
    "                sys.exit(1)\n",
    "\n",
    "            # Evaluation on all tasks (all strategies except MSTE)\n",
    "            if 'MSTE' not in s:\n",
    "                for j in range(num_tasks):\n",
    "                    eval_b = tasks_eval[j]\n",
    "                    print(f'Eval {j} - Len: {len(eval_b)}')\n",
    "                    eval_labels_b = labels_eval[j]\n",
    "                    print(f'{sum(1 for anom in eval_labels_b if anom == 1)}/{len(eval_b)} anomalies')\n",
    "\n",
    "                    preds_raw = clf.decision_function(eval_b)\n",
    "                    ROC = roc_auc_score(eval_labels_b, preds_raw)\n",
    "                    print(f'ROC: {ROC}')\n",
    "\n",
    "                    preds = clf.predict(eval_b)\n",
    "\n",
    "                    if not str(m).__contains__(\"contamination\"):\n",
    "                        np.place(preds, preds == 1, 0)\n",
    "                        np.place(preds, preds == -1, 1)\n",
    "\n",
    "                    # print(preds)\n",
    "                    # print(eval_labels_b)\n",
    "\n",
    "                    if eval_standard_metrics:\n",
    "                        cf = confusion_matrix(eval_labels_b, preds, labels=[0, 1])\n",
    "                        print(cf)\n",
    "\n",
    "                        [precision_micro, recall_micro, fscore_micro, support_RF_micro] = precision_recall_fscore_support(eval_labels_b, preds, average='micro')\n",
    "                        [precision_macro, recall_macro, fscore_macro, support_RF_macro] = precision_recall_fscore_support(eval_labels_b, preds, average='macro')\n",
    "                        [precision_weighted, recall_weighted, fscore_weighted, support_RF_weighted] = precision_recall_fscore_support(eval_labels_b, preds, average='weighted')\n",
    "                        print(f'{precision_micro},{recall_micro},{fscore_micro}')\n",
    "                        print(f'{precision_macro},{recall_macro},{fscore_macro}')\n",
    "                        print(f'{precision_weighted},{recall_weighted},{fscore_weighted}')\n",
    "                        mat_f1[i][j] = np.round(fscore_weighted, 2)\n",
    "\n",
    "                    mat_roc[i][j] = np.round(ROC, 2)\n",
    "\n",
    "        # Special out-of-loop evaluation (MSTE only)\n",
    "        if 'MSTE' in s:\n",
    "            for i in range(num_tasks):\n",
    "                for j in range(num_tasks):\n",
    "                    clf = MSTE[j]       # Select expert model from the pool\n",
    "                    eval_b = tasks_eval[j]\n",
    "                    print(f'Eval {j} - Len: {len(eval_b)}')\n",
    "                    eval_labels_b = labels_eval[j]\n",
    "                    print(f'{sum(1 for anom in eval_labels_b if anom == 1)}/{len(eval_b)} anomalies')\n",
    "\n",
    "                    preds_raw = clf.decision_function(eval_b)\n",
    "                    ROC = roc_auc_score(eval_labels_b, preds_raw)\n",
    "                    print(f'ROC: {ROC}')\n",
    "\n",
    "                    preds = clf.predict(eval_b)\n",
    "\n",
    "                    if not str(m).__contains__(\"contamination\"):\n",
    "                        np.place(preds, preds == 1, 0)\n",
    "                        np.place(preds, preds == -1, 1)\n",
    "\n",
    "                    # print(preds)\n",
    "                    # print(eval_labels_b)\n",
    "\n",
    "                    if eval_standard_metrics:\n",
    "                        cf = confusion_matrix(eval_labels_b, preds, labels=[0, 1])\n",
    "                        print(cf)\n",
    "\n",
    "                        [precision_micro, recall_micro, fscore_micro, support_RF_micro] = precision_recall_fscore_support(\n",
    "                            eval_labels_b, preds, average='micro')\n",
    "                        [precision_macro, recall_macro, fscore_macro, support_RF_macro] = precision_recall_fscore_support(\n",
    "                            eval_labels_b, preds, average='macro')\n",
    "                        [precision_weighted, recall_weighted, fscore_weighted,\n",
    "                         support_RF_weighted] = precision_recall_fscore_support(eval_labels_b, preds, average='weighted')\n",
    "                        print(f'{precision_micro},{recall_micro},{fscore_micro}')\n",
    "                        print(f'{precision_macro},{recall_macro},{fscore_macro}')\n",
    "                        print(f'{precision_weighted},{recall_weighted},{fscore_weighted}')\n",
    "                        mat_f1[i][j] = np.round(fscore_weighted, 2)\n",
    "\n",
    "                    mat_roc[i][j] = np.round(ROC, 2)\n",
    "\n",
    "        # matrices_f1[\"naive\"][str(m)] = mat_f1\n",
    "        # matrices_roc[\"naive\"][str(m)] = mat_roc\n",
    "\n",
    "        print(mat_roc)\n",
    "\n",
    "        l_roc = lifelong_roc(mat_roc)\n",
    "        bwt = backward_transfer(mat_roc)\n",
    "        fwt = forward_transfer(mat_roc)\n",
    "\n",
    "        heatmap(mat_roc, dataset_name, s, params[s], str(m), \"f\")\n",
    "        heatmap(mat_roc, dataset_name, s, params[s], str(m), \"b\")\n",
    "        heatmap(mat_roc, dataset_name, s, params[s], str(m), \"all\")\n",
    "\n",
    "        if \"AutoEncoder\" in str(m):\n",
    "            m = \"AutoEncoder\"\n",
    "\n",
    "        print('strategy,params,model,LROC,BWT,FWT')\n",
    "        print(f'{s},{params[s]},{m},{l_roc},{bwt},{fwt}')\n",
    "        logger.append(f'{s},{params[s]},{str(m).replace(\",\",\"\").replace(\" \",\"_\")},{l_roc},{bwt},{fwt}')\n",
    "\n",
    "        with open(f'logs/{dataset_name}_{s}_{params[s]}_{m}_rocmat.pkl', \"wb\") as fp:\n",
    "            pickle.dump(mat_roc, fp)\n",
    "\n",
    "    print(logger)\n",
    "\n",
    "    if os.path.isfile(f'logs/{dataset_name}_{s}_{params[s]}_metrics.csv'):\n",
    "        with(open(f'logs/{dataset_name}_{s}_{params[s]}_metrics.csv', \"a\")) as f:\n",
    "            for line in logger:\n",
    "                f.write(f'{line}\\n')\n",
    "        f.close()\n",
    "    else:\n",
    "        np.savetxt(f'logs/{dataset_name}_{s}_{params[s]}_metrics.csv', logger, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Load saved ROC matrix and print\n",
    "    with open(f'logs/{dataset_name}_{s}_{params[s]}_{m}_rocmat.pkl', \"rb\") as fp:\n",
    "        b = pickle.load(fp)\n",
    "        print(b)\n",
    "\n",
    "# TODO: naive, replay, and cumulative: train together or separately?\n",
    "# TODO: latex code for tables (acc, bwt, fwt) based on previous papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_metrics = pd.read_csv(\"logs/ccard_Naive__metrics.csv\")\n",
    "cumulative_metrics = pd.read_csv(\"logs/ccard_Cumulative__metrics.csv\")\n",
    "replay_metrics = pd.read_csv(\"logs/ccard_Replay_budget=0.2_compact_True_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>params</th>\n",
       "      <th>model</th>\n",
       "      <th>LROC</th>\n",
       "      <th>BWT</th>\n",
       "      <th>FWT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LocalOutlierFactor(n_neighbors=10_novelty=True)</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IsolationForest(random_state=0)</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy  params                                            model      LROC  \\\n",
       "0    Naive     NaN  LocalOutlierFactor(n_neighbors=10_novelty=True)  0.810667   \n",
       "1    Naive     NaN                  IsolationForest(random_state=0)  0.446667   \n",
       "\n",
       "     BWT    FWT  \n",
       "0 -0.018  0.802  \n",
       "1  0.096  0.447  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>params</th>\n",
       "      <th>model</th>\n",
       "      <th>LROC</th>\n",
       "      <th>BWT</th>\n",
       "      <th>FWT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cumulative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LocalOutlierFactor(n_neighbors=10_novelty=True)</td>\n",
       "      <td>0.831333</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cumulative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IsolationForest(random_state=0)</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     strategy  params                                            model  \\\n",
       "0  Cumulative     NaN  LocalOutlierFactor(n_neighbors=10_novelty=True)   \n",
       "1  Cumulative     NaN                  IsolationForest(random_state=0)   \n",
       "\n",
       "       LROC    BWT    FWT  \n",
       "0  0.831333  0.007  0.815  \n",
       "1  0.416667  0.013  0.462  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>params</th>\n",
       "      <th>model</th>\n",
       "      <th>LROC</th>\n",
       "      <th>BWT</th>\n",
       "      <th>FWT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Replay</td>\n",
       "      <td>budget=0.2_compact_True</td>\n",
       "      <td>LocalOutlierFactor(n_neighbors=10_novelty=True)</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Replay</td>\n",
       "      <td>budget=0.2_compact_True</td>\n",
       "      <td>IsolationForest(random_state=0)</td>\n",
       "      <td>0.443333</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy                   params  \\\n",
       "0   Replay  budget=0.2_compact_True   \n",
       "1   Replay  budget=0.2_compact_True   \n",
       "\n",
       "                                             model      LROC    BWT    FWT  \n",
       "0  LocalOutlierFactor(n_neighbors=10_novelty=True)  0.846000  0.018  0.822  \n",
       "1                  IsolationForest(random_state=0)  0.443333  0.017  0.479  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Replay\" strategy tends to perform the best across both models and consistently achieves high LROC scores, indicating good overall performance.\n",
    "\n",
    "Best Performing Model: The \"Local Outlier Factor (LOF)\" model generally outperforms the \"Isolation Forest\" model in terms of LROC across all strategies.\n",
    "\n",
    "Trade-offs: While \"Naive\" has a decent LROC for LOF, it experiences negative BWT, indicating some forgetting of the previous tasks. \"Replay\" strategy achieves a good balance between BWT and FWT.\n",
    "\n",
    "In summary, the \"Replay\" strategy with the Local Outlier Factor model appears to be the most effective for the given tasks based on the provided metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Local Outlier Factor model with the replay strategy demonstrated a substantial improvement in ROC-AUC, achieving a score of 0.846. This represents a significant advancement over one of the baseline models, Isolation Forest, which had an ROC-AUC score of 0.390. Ranking the models based on weighted precision scores from lowest to highest, the order is as follows: One-Class SVM (0.63), Isolation Forest (0.66), HBOS (0.685), COPOD (0.75), Local Outlier Factor (0.78), and the highest performing model, Local Outlier Factor with the replay strategy and continual learning.\n",
    "\n",
    "For future steps, it would be beneficial to explore additional anomaly detection algorithms and fine-tune hyperparameters to further enhance model performance. The observed success of Local Outlier Factor with the replay strategy may be attributed to its ability to adapt and learn from previous tasks, providing improved anomaly detection capabilities over standalone models."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d059f376a9d0551670ac739dcc834dd342b8d7d90019c6bdbef463e084516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
